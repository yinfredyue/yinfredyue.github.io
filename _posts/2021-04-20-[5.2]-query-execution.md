---
layout: single
categories: 
    - Database
author: Yue Yin

toc: true
toc_sticky: true
---



## TLDR

```
+--------------------+
| Query Planning     |
+--------------------+
| Operator Execution | <-
+--------------------+
|   Access Methods   |
+--------------------+
|Buffer Pool Manager |
+--------------------+
|   Disk Manager     |
+--------------------+
```

- Processing model (iterator, materialization, vectorization)
- Access model (sequential scan, index scan)
- Process model (process per worker, process pool, thread per worker)
- Inter-query parallelism 
- Intra-query parallelism (intra-operator, inter-operator, bushy)
- I/O parallelism (multi-disk parallelism, database partitioning, logical partitioning)



## Query Plan

The DBMS converts a SQL query into a *query plan*, which consists of operators arranged in a tree. Data flows from the leaf to the root. The output of the root node is the query result. **The same query plan can be executed in multiple ways.**



## Processing Models

A *processing model* defines how the DBMS executes a query plan. The model can be implemented to invoke operators either from top-to-bottom or from bottom-to-top. Although the top-to-bottom is more common, the bottom-to-top approach can allow tighter control of cache/register in pipelines.

### Iterator Model

Also known as Volcano or Pipeline model. The most common processing model for row-based DBMS. The iterator model works by implementing a `next` function for every operator. Each node in the query plan calls `next` on its children until the leaf nodes are reached, which start emitting tuples to the upper levels. Each tuple is then processed up the plan tree as far as possible before the next tuple is retrieved. This is useful in disk-based systems because it allows the DBMS to fully use each tuple in memory before the next tuple/page is accessed. Some operators will block until children emit all their tuples (joins, order by). These are called *pipeline breakers*.

### Materialization Model

The *materialization model* is a specialization of the iterator model where each operator processes all its input before emitting all at once - the operator *materializes* its output as a single result. To avoid scanning tuples wastefully, the DBMS can propagate down information to subsequent operators about how many tuples are needed. Each query plan operator implements an `output` function. 

The materialization model is better for OLTP, where queries access a small number of tuples that can fit into memory completely, and there are fewer function calls than the iterator model. The materialization model is not suitable for OLAP queries with large intermediate results, as the DBMS may have to spill to disk between operators.

### Vectorization Model

The *vectorization model* has each operator implements a `next` function that returns a *batch* (or vector) of data instead of a single tuple. The vectorization model is ideal of OLAP queries that have to scan a large number of tuples as there are fewer calls to `next`.



## Access Methods

An *access method* is how the DBMS accesses the data in a table. There're two major approaches: (1) sequential scan; (2) index scan. 

### Sequential Scan

The sequential scan operator iterates over every page in the table and retrieves it from the buffer pool. 

### Index Scan

The goal of *index scan* is to pick the index that help find the data most efficiently. Some DBMSs support multi-index scan: the DBMS computes sets of record IDs using each matching index, combine the sets based on the query predicate, retrieve the records, and apply the remaining predicates. The set intersection can be done by bitmaps, hashtables, or Bloom filters.



## Process Models

A *process model* defines how the system supports concurrent requests. There're three models:

- Process per worker: fork a process for each request. Relies on OS scheduling and shared-memory.
- Process pool: save fork cost. Relies on OS scheduling and shared-memory. 
- Thread per worker: multiple worker threads in a single process. The DBMS can manage its own scheduling.



## Inter-Query Parallelism

In *inter-query parallelism*, the DBMS executes different queries concurrently. If the queries are read-only, little coordination is needed. For writing queries, concurrency control is needed. We discuss this topic in a later post.



## Intra-Query Parallelism

In *intra-query parallelism*, the DBMS executes operators of a single query in parallel. This reduces latency for long-running queries. 

The organization of intra-query parallelism is similar to a *producer/consumer* paradigm: each operator consumes data from operators below it, and produces data for operators above it.

Parallel algorithms exists for every relational operator. DBMS can either have multiple threads access centralized data structure, or use partitioning to divide work up. 

In intra-query parallelism, there're three types of parallelism: intra-operator, inter-operator, and bushy. They are not mutually exclusive. 

### Intra-Operator Parallelism (Horizontal)

### Inter-Operator Parallelism (Vertical)

### Bushy Parallelism



## I/O Parallelism

Parallel execution will not improve performance if the disk is the bottleneck. In this case, we need to split a database across multiple storage devices. There're three approaches: multi-disk parallelism, database partitioning, and logical partitioning. 

### Multi-disk Parallelism

In *multi-disk parallelism*, the OS stores the DBMS's files across multiple storage devices (like RAID). This is transpraent to the DBMS. 

### Database Partitioning

In *database partitioning*, the database is split into disjoint subsets that can be assigned to concrete disks. Some DBMS allow you to specify the disk location of each individua database.

### Logical Partitioning

In *logical partitioning*, a single logical table is split into disjoint physical segments, that are stored separately. The partition is transparent to the application. *Vertical partitioning* is like a column store. *Horizontal partitioning* divides tuples into disjoint segments based on some partitioning keys (hash, range, predicate partitioning, etc). 