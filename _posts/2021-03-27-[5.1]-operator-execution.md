---
layout: single
categories: 
    - Database
author: Yue Yin

toc: true
toc_sticky: true
---

## TLDR

```
+--------------------+
| Query Planning     |
+--------------------+
| Operator Execution | <-
+--------------------+
|   Access Methods   |
+--------------------+
|Buffer Pool Manager |
+--------------------+
|   Disk Manager     |
+--------------------+
```

This post explains implementations for sorting, aggregation, and joins.

Sorting: in-memory, external, using clustered index.

Aggretaion: sorting, hashing. 

Join: nested joins, sort-merge join, hash join.


## Sorting

Tuples in a table have no specific order under the relation model, but `ORDER BY`, `GROUP BY`, `DISTINCT` requires sorting. If the data fits into memory, any standard sorting algorithm (e.g. quicksort) works. Otherwise, the DBMS uses some algorithm that can spill to disk and prefers sequential disk I/O.

The standard algorithm is **external merge sort**. 
- **Phase #1 - Sorting**: sorts small chunks of data that fits in memory, and writes sorted chunks to disk (called *runs*).
- **Phase #2 - Merge**: combines sorted runs into a larger single file (like `merge` in merge-sort). 

One optimization is *double buffering*, which prefetches the next run into a second buffer while the system is processing the current run. This requires multi-threading and reduces waiting for disk I/O.

If there's a clustered index, the DBMS can just traverse the index and it guarantees sequential I/O. However, using non-clustered index will cause random I/O. 

Assume we have $B$ buffers. Cost: $ 2N \times (1 + \lceil \log_{B-1} \lceil \frac{N}{B} \rceil \rceil) $, where $N$ is the number of blocks.

## Aggregation

There are two approaches for implementing an aggregation: (1) sorting and (2) hashing. 

### Sorting

Sort the tuples on the `GROUP BY` keys, using in-memory sort, external merge sort, or an clustered index. Then
perform a sequential scan over the sorted data to perform aggregation. 

### Hashing

Hashing can be computationally cheaper than sorting for computing aggregations. The DBMS populates a emhemeral hash table as it sequentially scans the table. If the size of hash table cannot fit into memory, the DBMS has to spill it to disk. This involves two phases:

- **Phase #1 - Partition**: Use a hash function `h1` to split tuples into partitions on disk (assume each partition fits into memory).
- **Phase #2 - ReHash**: for each partition, read it into memory and build an in-memory hash table using a second hash function `h2`. Then go through each slot in this hash table to compute the aggregation. 


## Joins

We only discuss *innerjoin*s. [What's INNER JOIN, LEFT JOIN, RIGHT JOIN, and OUTER JOIN?](https://stackoverflow.com/a/38578/9057530).

### Operator Output

For a tuple $r \in R$ and a tuple $s \in S$ that match on join attributes, the join operator concatenates $r$ and $s$ together into a new output triple. There are two formats to return the conent:

- **Data**: copy the values in the outer and inner tables into into a new output tuple. 
    - Pro: future operators don't need to go back to the base tables.
    - Con: this requires more memory to materialize the entire tuple. 

- **Record IDs**: only copies copied join keys and the record IDs of the matching tuples. This is called *late materialization*. 
    - Pro: reduce memory usage for materialization.
    - Con: future operatos need to retrieve value from the base table again. 

### Cost Analysis

The cost counts the number of disk I/Os. This includes I/O for reading data from disk and writing any *intermediate* data out to disk. This does not include I/O for outputting the result. 

Variables used:
- M pages in the table R (Outer table), m tuples
- N pages in the table S (Inner table), n tuples

### Nested Loop Join

- Simple Nested Loop Join: $ M + (m \times N) $
- Block Nested Loop Join
    - With 3 buffer pages: $ M + (M \times N) $.
    - With B buffer pages (use $ B-2 $ buffers for outer table): $ M + (\lceil \frac{M}{B-2} \rceil \times N) $.
- Index Nested Loop Join: $ M + (m \times C) $, where $C$ is the cost for index probe.

### Sort-Merge Join

Sort the two tables on the join keys, and merge using two pointers. 

This algorithm is useful if one or both of the two tables are sorted, or if the output needs to be sorted on the join key. 

Assume the DBMS has $B$ buffers:
- Sort outer table: $ 2M \times (1 + \lceil \log_{B-1} \lceil \frac{M}{B} \rceil \rceil) $
- Sort inner table: $ 2N \times (1 + \lceil \log_{B-1} \lceil \frac{N}{B} \rceil \rceil) $
- Merge: $M + N$


### Hash join

#### Basic Hash Join

- Phase #1 - Build: scan the outer relation and populate a hash table using the hash function `h1`. The key is the join attributes. The value can be full tuple values or a tuple id.
- Phase #2 - Probe: Scan the inner relation and use the hash function `h1` to jump to corresponding slot in the hash table. This requires a sequential scan of tuples in the slot.

<img src="{{ site.url }}/assets/images/basic_hash_join.png" alt="Basic hash join"/>

Assume the table fits into memory buffer. Cost: $M + N$. 

#### Grace Hash Join

When the table does not fit into memory, the basic hash join algorithm needs to swap the table in and out randomly, having poor performance. The Grace Hash Join mitigates the problem:

- Phase #1 - Build: scan <u>both</u> tables and populate a hash table using `h1`. If a single bucket of the hash table doesn't fit into memory, use *recursive partitioning* with differnt hash function `h2` to split the bucket. 
- Phase #2 - Probe: for each bucket level, retrive the pages for both outer and inner tables. Perfom a nested loop join on the tuples. In-memory join is very fast.

<img src="{{ site.url }}/assets/images/grace_hash_join.png" alt="Grace hash join"/>

<img src="{{ site.url }}/assets/images/recursive_partition.png" alt="Recursive partition"/>

Cost:
- Partition: $2 \times (M + N)$
- Probe: $(M + N)$

