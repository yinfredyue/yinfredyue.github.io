---
layout: single
categories: 
    - Database
author: Yue Yin

toc: true
toc_sticky: true
---


## Transactions

Goal: correctness + efficiency.

ACID:

- atomicity: either all actions in the transaction happen, or none happen
- consistency: if the transaction is consistent and the database is consistent at the beginning of the transaction, then the database is guaranteed to be consistent when the transaction completes.
- isolation: when a transaction executes, it should have the illusion that it is isolated from other transactions.
- durability: if a transaction commits, its effect on the database should persist.

### Atomicity

There are two approaches: Logging, and Shadow Paging. 

In logging, the DBMS logs all actions so that it can undo the actions of aborted transactions. Most widely used.

In shadow paging, the DBMS makes copies of pages modified by the transactions and transactions make changes to those copies.Only when the transaction commits is the page made available. Compared with logging, this is slower at runtime but faster at recovery. 

### Consistency

Logical correctness of the database. 

### Isolation

The DBMS provides the illusion that they are running alone in the system, and do not see the effects of concurrent transactions. This is **equivalent** to a system where transaction execute in **serial order** but with better performance. The DBMS must interleave the operations of concurrent transactions while maintaining the illusion of isolation.

A *concurrency control protocol* is how the DBMS decides the proper interleaving of operations in concurrent transactions at runtime. There are two categories of concurrency control protocols: **pessimistic** and **optimistic**. 

The order in which the DBMS executes operations is called *execution schedule*. The goal of concurrency control protocol is to generate an execution schedule equivalent to some serial execution: 

- Serial schedule: no interleaving happens.
- Equivalent schedule: two schedules are equivalent if the effect of executing the first schedule is identical to the effect of executing the second schedule, for any database state.
- Serializable schedule: a serializable schedule is a schedule equivalent to any serial schedule.

A *conflict* between two operations occur if the operations are from different transactions, being performed on the same object, and at least one of them is a write. There are three types of conflicts:

- Read-write conflicts (unrepeatable read): a transaction see different values when reading an object multiplt times.
- Write-read conflicts (dirty read): a transaction sees the write effects of an uncommitted transaction
- Write-write conflict (lost update): a transaction overwrites the uncommited writes by another concurrent transaction.

DBMS typically supports *conflict serializability*. Two schedules are *conflict equivalent* is they involve the same operations of the same transactions, and every pair of conflicting operations is ordered in the same way in both schedules. A schedule is *conflict serializable* if it is conflict equivalent to some serial schedule.

One can verify that a schedule is conflict serializable by swapping non-conflict operations until a serial schedule is formed. For schdules with many transactions, this can be expensive. A better way is to use a *dependency graph*. 

In a dependency graph, each transaction is a node. A directed edge from node T1 to node T2 exists if and only if an operation O1 in N1 conflicts with an operation O2 in N2, and O1 occurs before O2 in the schedule. The schedule is conflict serializable if the dependency graph is acyclic.

### Durability

All changes of committed transactions must be durable, even after a crash or restart. Method: logging, shadow paging.



## Two-Phase Locking (2PL)

A dependency graph tells you if a existing schedule is serializable, but cannot generate the schdule for you at runtime. A DBMS can use locks to dynamically generate serializable schedule for transactions at runtime. A centralized transaction manager maintains a lock table to decide whether a transaction can acquire a lock or not. The lock table doesn't need to be durable as any active transaction when the DBMS crashes is automatically aborted.

Two-phase locking (2PL) is a pessimistic concurrency control protocol. In the growing phase, a transaction acquires locks. The transaction enters the shrinking phase immediately after it releases the first lock. The transaction cannot acquire locks any more in the shrinking phase.

A schedule is *strict* if any value written by a transaction is not read/overwritten by another transaction until the first transaction commits. *Strict 2PL* only releases locks on commit. The advantage strict 2PL is that it doesn't incur cascading aborts. The disadvantage is that it limits concurrency even more.

### Deadlock Handling

There are two appraches to handle deadlocks: detection and prevention.

In *deadlock detection*, the DBMS creates a waits-for graph where transactions are nodes and there exists a directed edge from Ti to Tj if transaction Ti is waiting for transaction Tj to release a lock. The DBMS can launch a background thread to periodically check for cycles in the waits-for graph and break the cycles by selecting "victim" transaction(s) to abort. The victim transaction will either restart or abort depending on the application. Multiple transaction properties can be considered when selecting a victim.

Instead of letting transactons to acquire any lock they need and deal with deadlocks afterwards, *deadlock prevention* 2PL stops transactions from causing deadlocks before they occur. When a transaction tries to acquire a lock held by another transaction (which could cause a deadlock), the DBMS kills one of them. To implement this, transactions are assigned priorities based on timestamps (older transactions have higher priority). The schems guarantee no deadlocks as only one direction is allowed when waiting for a lock. When a transaction restarts, it uses the ***original*** timestamp, to avoid starvation.

### Lock granularity

Tuple-level lock is very expensive. Hierarchical locking reduces the overhead. For example, a lock on the table is much cheaper than locking all tuples in the table. **Intention locks** allow a higher level node to be locked in shared/exclusive mode without checking all decendant nodes. 



## Timestamp Ordering

Timestamp ordering (T/O) is a class of optimistic concurrency control protocols. Two properties: 

- Each transacton Ti is assigned a *unique*, *fixed* timestamp that's monotonically increasing. 
- If TS(Ti) < TS(Tj), the execution schedule is equivalent to a serial schedule where Ti appears before Tj.

To allocate timestamps, you can use system clock, but with edge cases like daylight savings. You can also use logical counter, but must deal with overflows. Some systems use hybrid approach. Also, maintaining the timestamp across a distributed system is hard.

### Basic T/O

No locks are used. Every database object X is tagged with a timestamp of the last transaction that successfully perform a read (R-TS(X)) or write (W-TS(X)). The DBMS checks the timestamp for every operation. If a transaction tries to access an object in a way that violates the timestamp ordering, the transaction is aborted and restarted.

For read, the ordering is violated if TS(Ti) < W-TS(X), so Ti is restarted with the same timestamp. Otherwise, the read is performed and R-TS(X) is updated to be max{R-TS(X), TS(Ti)}. It makes a local copy of X to ensure repeatable read.

For write, if TS(Ti) < R-TS(X) or TS(Ti) < W-TS(X), Ti must be restarted with a new timestamp. Otherwise, the write is performed and W-TS(X) is updated to be TS(Ti). It makes a local copy of X to ensure repeatable read.

Possible problems: (1) high overhead from making copies for repeatable read; (2) long running transactions can get starved; (3) suffers from timestamp allocation bottleneck on highly concurrent system.

### Optimistic Concurrency Control (OCC)

Optimistic concurrency control (OCC) is another optimistic concurrency control protocol that uses timestamps. In OCC, a *private workspace* is created for each DBMS. Any object read is copied into the workspace, and any object written is copied into the workspace and modified there. No other transactions can read the changes made by a transaction in its private workspace. 

When a transaction commits, the DBMS compares the transaction's workspace *write set* to see whether it conflicts with other transactions. If there's no conflicts, the write set is installed into the "global" database. OCC consists of 3 phases:

- Work phase: the DBMS tracks read/write sets of transactions and store writes in a private workspace;
- Validation phase: before a transaction commits, the DBMS checks if it conflicts with other transactions. 
- Write phase: if validation succeeds, apply the private workspace changes to the database. Otherwise, restart txn.

The DBMS assigns transaction timestamps when they enter the validation phase. Transaction that havn't entered the validation phase are assigned a timestamp of infinity. To ensure serializable schedule, the DBMS checks Ti against other transactions for RW or WW conflicts and makes sure that all conflict are in one direction (e.g older -> younger). There're two methods for this phase: backward validation, and forward validation. Backward validation checks if the read/write set of commiting transaction intersects with those of the committed ones. Forward validation checks if the read/write set of commiting transaction intersects with those not yet committed.

If TS(Ti) < TS(Tj), then one of the following three conditions must hold to pass the validation phase:

- Ti completes all three phases before Tj begins;
- Ti completes before Tj starts its write phase, and Ti does not write to any object read by Tj (Suppose Ti doesn't abort and its write set intersects with Tj, Tj's write could depend on the stale value, and later overwrites the value written by Ti. This is incorrect - a transaction with larger timestamp should not miss the writes by another transaction with lower timestamp);
- Ti completes Work phase before Tj completes its Work phase, and Ti doesn't write to any object read/written by Tj.

Possible problems: (1) high overhead for copying data locally into private workspace; (2) validation/write phase bottleneck (latching needed to avoid data race); (3) aborts are wasteful; (4) timestamp allocation bottleneck.



## Ioslation Levels

| Isolation levels                     | Behavior                                        |
| ------------------------------------ | ----------------------------------------------- |
| read uncommitted                     | all anomalies                                   |
| read committed                       | no dirty read                                   |
| repeatable read (snapshot isolation) | no dirty read, no unrepeatable read             |
| serializable                         | no dirty read, no unrepeatable read, no phantom |

How to avoid phantom read? What's the difference from nonrepeatable read?: https://stackoverflow.com/a/23138849

```
                    Implementation
Serializable         strict 2PL; index lock;
Repeatable read      strict 2PL (S-locks released on commit/abort)
Read committed       strict 2PL; acquiring/releasing S-locks doesn't affect growing/shrinking phase, and S-locks are
                     immediately released after each read (and re-acquired later if needs to read).
Read uncommitted     strict 2PL; not acquiring S-locks for read
```



## Multi-Version Concurrency Control

**MVCC is the most widely used scheme in DBMS**, it's more than a concurrency control protocol. It's mainly about determining the visibility of writes to transactions.

With MVCC, the DBMS mtaintains multiple **physical** version of a single **logical** object. When a transaction writes to an object, the DBMS creates a new version of that object. When a transaction reads an object, it reads the newest version that existed when the transaction started. It won't see any writes that are not committed yet when the transaction starts. For detailed explanation of MVCC execution and the usage of transaction status table, refer to Lecture 19 slides.

The fundamental concept/benefit of MVCC is that writes do not block writers, and readers do not block writers. This means one transaction can modify an object while other transactions read old versions. Also, read-only transactions can read a consistent snapshot of the database without using any locks. 

There are four important MVCC design decisions:

- Concurrency Control Protocol
- Version Storage
- Garbage Collection
- Index Management

### Concurrency Control Protocol

For concurrency control protocl, you can choose among 2PL, T/O and OCC. 

### Version Storage

Version storage is about how the DBMS stores different physical versions of a logical object and how transactions find the newest version visible to them. The DBMS uses the tuple's pointer field to create a *version chain* per logical tuple, which is a linked list of versions sorted by timestamp. Indexes alwasy point to the "head" of the chain, which is either the newest or oldest version depending on implementation. A thread traverses the chain until it finds the correct version.

Different storage schemes determine where/what to store for each version:

- **Append-only storage**. All physical versions of a logical tuple are stored in the same table space. Versions are mixed together in the table and each update appends a new version of the tuple into the table, and updates the version chain.

  ![image-20210423132114894](/home/yy0125/.config/Typora/typora-user-images/image-20210423132114894.png)

- **Time-travel storage**. The DBMS maintains a sperate table called the time-travel table to store older versions of the tuple. On every update, the DBMS copies the old version of the tuple to the time-travel table, and overwrites the tuple in the main table with the new data. Pointers of the tuple in the main table point to old versions in the time-travel table.

  ![image-20210423132342010](/home/yy0125/.config/Typora/typora-user-images/image-20210423132342010.png)

- **Delta storage**. Delta storage uses a separate table similar to time-travel storage, but only stores the deltas (changes between tuples). Transactions can recreate older versions by iterating through the deltas. This has faster write than time-travel storage, but slower reads.

### Garbage collection

The DBMS remove *reclaimable* physical versions from the database over time. A version is reclaimable if no active transaction can see that version, or if it was created by an aborted transaction.

For *tuple-level GC*, the DBMS finds old versions by scanning tuples directly. This can be done by a background thread periodically, A simple optimization is to maintain a "dirty page bitmap", which keeps track of the pages that have been modified since previous scan. This help the thread to skip pages that havn't changed. Another approach is for worker threads to identify reclaimable versions as they traverse the version chain.

For *transaction-level GC*, each transaction is responsible for tracking their own old versions so that DBMS doesn't have to scan tuples. Each transaction maintains its own read/write set. When it commits/aborts, the DBMS can use that to identify which tuples to reclaim.

### Index Management

All primary key indexes always point to the version chain head. 





















